# 智能助手视觉避障与路径导航需求说明书 (PRD)

## 1. 项目背景
为视力障碍人士提供一款智能手机 Agent，在现有语音操控（AutoGLM）基础上，增加真实世界的感知能力，解决用户出行中的“路在哪”与“怎么走”的问题。

## 2. 核心功能需求

### 2.1 视觉避障 (Local Path Planning)
*   **场景描述**: 在行走过程中，实时检测前方障碍物。
*   **功能点**:
    *   **后台图像采集**: Android 端在用户开启导航后，静默调用摄像头（不显示预览界面）。
    *   **障碍物识别**: 识别台阶、路灯、违停车辆、施工围栏、盲道占用等。
    *   **语音反馈**: 根据障碍物距离和方位，提供即时语音提醒（如：“前方三米有台阶，请减速”）。

### 2.2 路径导航与语音导引 (Global Path Planning & TTS)
*   **场景描述**: 用户通过语音输入目的地，系统获取路径并进行文字化播报。
*   **功能点**:
    *   **导航 API 集成**: 调用高德/百度地图的步行路径规划 API。
    *   **文字指令获取**: 提取 API 返回结果中的 `instruction` 字段（如：“沿学院路走 200 米，向左转”）。
    *   **指令“盲人化”加工**: 
        *   后端 Agent 将绝对方向（东南西北）根据手机朝向转化为相对方向（左转、右前方等）。
        *   将枯燥的“XXX米”转化为更有感的描述（如：“大约走 30 步”）。
    *   **TTS 语音播报**: Android 端接收到加工后的文字指令，调用系统的 `TextToSpeech` 引擎进行即时播报。

### 2.3 “最后十米”精准引导
*   **场景描述**: 到达目的地附近后，寻找具体的门牌或入口。
*   **功能点**:
    *   **视觉确认**: 利用多模态大模型识别特定的标志物（如“XX便利店”、“地铁 A 口”）。
    *   **方位补偿**: 结合手机传感器，通过持续的语音方位提示（如：“目标在您 2 点钟方向，约 5 米”）引导用户。

## 3. 技术架构设计

### 3.1 模块划分
*   **Android Client**: 
    *   **ImageCaptureManager**: 后台图像采集。
    *   **TTSManager**: 封装 Android `TextToSpeech` 类，负责文字转语音播报。
    *   **SensorManager**: 获取指南针/陀螺仪数据用于方向校正。
*   **Spring Boot (Control Plane)**: 负责对接导航 API，管理用户导航状态。
*   **FastAPI (Model Plane)**:
    *   **Vision Agent**: 环境理解。
    *   **Linguistic Agent**: 负责将导航 API 的原始文字指令（Instruction）重写为更符合盲人习惯的导引词。

### 3.2 交互流程
1.  **用户**: “我想去五道口地铁站。”
2.  **App**: 启动导航 API 获取路径；提取第一条 `instruction`。
3.  **Linguistic Agent**: 将“沿成府路向东走 50 米”重写为“请直行，走大约 60 步”。
4.  **App (TTS)**: 语音播报：“请直行，走大约 60 步”。
5.  **视觉检测**: 如果发现路中间有障碍物，立即插播避障语音。

## 4. 推荐开发路线 (Roadmap)

### 第一阶段：基础构建 (1-2周)
*   **Android**: 实现基于 CameraX 的静默抓拍；集成 TTS 基础播报功能。
*   **Backend**: 跑通高德/百度步行导航 API，提取文字指令。

### 第二阶段：指令优化与避障融合 (2-3周)
*   **Agent Logic**: 开发指令重写逻辑（绝对方向转相对方向）。
*   **Fusion**: 实现导航指令与视觉避障指令的优先级调度（避障指令优先级最高）。

### 第三阶段：专项优化 (1-2周)
*   **Precision**: 利用视觉 Agent 进行目的地特征点匹配，解决“最后十米”定位漂移问题。

### 第四阶段：实地测试
*   针对复杂路口、无盲道区域进行播报频率调优。
